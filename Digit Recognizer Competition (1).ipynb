{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib as mtl, zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class, which unpacks files from zip, rar archives\n",
    "class Unziper:\n",
    "    def __init__(self,path):\n",
    "        self.path=path\n",
    "        self.check_path=self.__unpacker()\n",
    "   \n",
    "    def __unpacker(self):\n",
    "        def chech_path_existance(self):\n",
    "            if os.path.exists(self.path):\n",
    "                def file_extracter(self):\n",
    "                    list_to_unpack=os.listdir(self.path)\n",
    "                    if list_to_unpack:\n",
    "                        other_formats_list=[]\n",
    "                        for file in list_to_unpack:\n",
    "                            if re.findall('\\.(zip)|(rar)',file):\n",
    "                                with zipfile.ZipFile(os.path.join(self.path,file)) as zf:\n",
    "                                    zf.extractall(self.path)\n",
    "                                result_to_unpack='Upacked'\n",
    "                            else:\n",
    "                                other_formats_list.append(file)\n",
    "                        if len(list_to_unpack)==len(other_formats_list):\n",
    "                            result_to_unpack='Nothing to unpack'\n",
    "                    else:\n",
    "                        result_to_unpack='Nothing to unpack'\n",
    "                    return result_to_unpack\n",
    "                return file_extracter(self)\n",
    "            else:\n",
    "                result='Path does not exist'\n",
    "                return result\n",
    "            \n",
    "        return chech_path_existance(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_unziper=Unziper(r'C:\\Users\\pc\\Desktop\\ML_projects\\Digit Recognizer Competition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Upacked'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_unziper.check_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class, which creates dictionaries of files\n",
    "class FileReader:\n",
    "    def __init__(self,path):\n",
    "        self.path=path\n",
    "        self.read_flat_file=self.create_dict()\n",
    "        \n",
    "    def create_dict(self):\n",
    "        if os.listdir(self.path):\n",
    "            dict_={}\n",
    "            for file in os.listdir(self.path):\n",
    "                if re.findall('\\.csv',file):\n",
    "                    read_file=pd.read_csv(os.path.join(self.path,file))\n",
    "                    key=re.findall('(\\w+)\\.',file)[0]\n",
    "                    dict_[key]=read_file\n",
    "            if len(dict_):\n",
    "                return dict_\n",
    "            else:\n",
    "                result='Nothing to read'\n",
    "        else:\n",
    "            result='Nothing to read'\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_reader=FileReader(r'C:\\Users\\pc\\Desktop\\ML_projects\\Digit Recognizer Competition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=file_reader.read_flat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission,test,train=files['sample_submission'],files['test'],files['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Datat preparation</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class, which shuffles and splits dataset\n",
    "class ShuffleSplit:\n",
    "    def __init__(self,random_state,validation_size):\n",
    "        self.random_state=random_state\n",
    "        self.validation_size=validation_size\n",
    "    \n",
    "\n",
    "    def __split(self,shuffled_dataset):\n",
    "        upper_border=int(np.ceil(len(shuffled_dataset)*(1-self.validation_size)))\n",
    "        train_dataset=shuffled_dataset[:upper_border]\n",
    "        validation_dataset=shuffled_dataset[upper_border:]\n",
    "        return train_dataset,validation_dataset\n",
    "            \n",
    "    def shuffle_split(self,dataset):\n",
    "        new_order=np.random.RandomState(seed=self.random_state).permutation(dataset.shape[0])\n",
    "        new_dataset=dataset.values[new_order]\n",
    "        \n",
    "            \n",
    "        return self.__split(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_split=ShuffleSplit(random_state=42,validation_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset,validation_dataset=shuffle_split.shuffle_split(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_no_labels,validation_dataset_no_labels,train_dataset_labels,validation_dataset_labels=train_dataset[:,1:],validation_dataset[:,1:],train_dataset[:,0].reshape(-1,1),validation_dataset[:,0].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33600, 784), (8400, 784), (33600, 1), (8400, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_no_labels.shape,validation_dataset_no_labels.shape,train_dataset_labels.shape,validation_dataset_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class, which normilizes or standardizes datasets depending on the strategy chosen \n",
    "class Scaler:\n",
    "    def __init__(self,strategy):\n",
    "        if strategy=='Normalization' or strategy=='Standardization':\n",
    "            self.__strategy=strategy\n",
    "        else:\n",
    "            raise Exception ('There is no such strategy yet')\n",
    "            \n",
    "    def scale(self,train_dataset=False,test_dataset=False):\n",
    "        if np.any(train_dataset) and not np.any(test_dataset):\n",
    "            calculations=self.__make_calculations_on_train(train_dataset)\n",
    "            return calculations\n",
    "        elif np.any(test_dataset) and not np.any(train_dataset):\n",
    "            calculations=self.__make_calculations_on_test(test_dataset)\n",
    "            return calculations\n",
    "        else:\n",
    "            raise Exception('Train or test datasets should be passes')\n",
    "        \n",
    "             \n",
    "    def __make_calculations_on_train(self,train_dataset):       \n",
    "        if self.__strategy=='Normalization':\n",
    "            self.train_dataset_min=train_dataset.min(axis=0)\n",
    "            self.train_dataset_max=train_dataset.max(axis=0)\n",
    "            result=(train_dataset-self.train_dataset_min)/(self.train_dataset_max-self.train_dataset_min)\n",
    "            return np.nan_to_num(result)\n",
    "        elif self.__strategy=='Standardization':\n",
    "            self.train_dataset_mean=train_dataset.mean(axis=0)\n",
    "            self.train_dataset_std=train_dataset.std(axis=0)\n",
    "            result=(train_dataset-self.train_dataset_mean)/self.train_dataset_std\n",
    "            return np.nan_to_num(result)\n",
    "            \n",
    "    \n",
    "    def __make_calculations_on_test(self,test_dataset):\n",
    "        if self.__strategy=='Normalization':\n",
    "            result=(test_dataset-self.train_dataset_min)/(self.train_dataset_max-self.train_dataset_min)\n",
    "            return np.nan_to_num(result)\n",
    "        elif self.__strategy=='Standardization':\n",
    "            result=(test_dataset-self.train_dataset_mean)/self.train_dataset_std\n",
    "            return np.nan_to_num(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_norm=Scaler(strategy='Normalization')\n",
    "scaler_stand=Scaler(strategy='Standardization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "train_dataset_no_labels_normolized=scaler_norm.scale(train_dataset=train_dataset_no_labels)\n",
    "train_dataset_no_labels_standardized=scaler_stand.scale(train_dataset=train_dataset_no_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "validation_dataset_no_labels_normolized=scaler_norm.scale(test_dataset=validation_dataset_no_labels)\n",
    "validation_dataset_no_labels_standardized=scaler_norm.scale(test_dataset=validation_dataset_no_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Making predictions</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The class below runs cross_validation, fits models, predicts target labels\n",
    "class FitPredictorEstimator:\n",
    "    def __init__(self,prediction_strategy,estimation_strategy,specify_estimation_strategy=False,specify_random_state=False,specify_cv=False,specify_num_estimators=False,specify_max_depth=False):\n",
    "        self.__prediction_strategy=prediction_strategy\n",
    "        self.__estimation_strategy=estimation_strategy\n",
    "        self.__specify_estimation_strategy=specify_estimation_strategy\n",
    "        self.__specify_random_state=specify_random_state\n",
    "        self.__specify_cv=specify_cv\n",
    "        self.__specify_num_estimators=specify_num_estimators\n",
    "        self.__specify_max_depth=specify_max_depth\n",
    "        \n",
    "        \n",
    "        if self.__prediction_strategy=='sgd_classifier':\n",
    "            from sklearn.linear_model import SGDClassifier\n",
    "            if not self.__specify_random_state:\n",
    "                self.__model=SGDClassifier()\n",
    "            else:\n",
    "                self.__model=SGDClassifier(random_state=self.__specify_random_state)\n",
    "        elif self.__prediction_strategy=='random_forest_classifier':\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            if not self.__specify_num_estimators and not self.__specify_max_depth:\n",
    "                self.__model=RandomForestClassifier()\n",
    "            elif self.__specify_num_estimators and self.__specify_max_depth:\n",
    "                self.__model=RandomForestClassifier(n_estimators=self.__specify_num_estimators,max_depth=self.__specify_max_depth)\n",
    "            elif self.__specify_num_estimators and not self.__specify_max_depth:\n",
    "                self.__model=RandomForestClassifier(n_estimators=self.__specify_num_estimators)\n",
    "            elif not self.__specify_num_estimators and self.__specify_max_depth:\n",
    "                elf.__model=RandomForestClassifier(max_depth=self.__specify_max_depth)\n",
    "        else:\n",
    "            raise Exception ('There is no such strategy yet')\n",
    "            \n",
    "    def cross_validate_estimate(self,train_dataset,train_labels):\n",
    "        if self.__specify_cv:\n",
    "            from sklearn.model_selection import cross_val_predict\n",
    "            self.predicted_train_labels=cross_val_predict(self.__model,train_dataset,train_labels,cv=self.__specify_cv)\n",
    "            self.cross_validate_estimation=self.__estimate(train_labels,self.predicted_train_labels)\n",
    "        else:\n",
    "            raise Exception ('Method works with k-folds cross validation, please, use specify_cv, while creating an object of Class')\n",
    "            \n",
    "            \n",
    "    def __estimate(self,labels,predicted_labels):\n",
    "        if self.__estimation_strategy=='f1_score':\n",
    "            from sklearn.metrics import f1_score\n",
    "            return f1_score(labels,predicted_labels,average=self.__specify_estimation_strategy)\n",
    "        else:\n",
    "            raise Except ('There is no such strategy yet')\n",
    "            \n",
    "            \n",
    "    def fit(self,train_dataset,train_labels):\n",
    "        self.__model.fit(train_dataset,train_labels)\n",
    "        \n",
    "    def predict(self,dataset):\n",
    "        self.__predictions=self.__model.predict(dataset)\n",
    "        return self.__predictions\n",
    "        \n",
    "    def estimate(self,validation_labels):\n",
    "        self.validation_estimation=self.__estimate(validation_labels,self.__predictions)\n",
    "        return self.validation_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_estimator_normalized_linear=FitPredictorEstimator(prediction_strategy='sgd_classifier',\n",
    "                                         estimation_strategy='f1_score',\n",
    "                                         specify_random_state=42, specify_estimation_strategy='weighted',specify_cv=5\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_estimator_normalized_linear.cross_validate_estimate(train_dataset_no_labels_normolized,train_dataset_labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904743495917294"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's calculate estimations on normolized training set\n",
    "predictor_estimator_normalized_linear.cross_validate_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Well, not too bad, let's now train our model and make predictions on validation set\n",
    "predictor_estimator_normalized_linear.fit(train_dataset_no_labels_normolized,train_dataset_labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 2, 4, ..., 2, 6, 0], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_estimator_normalized_linear.predict(validation_dataset_no_labels_normolized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9098029276790895"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Our model can generalize, since the estimation on validation set is approximately the same as on the training set and even higher \n",
    "predictor_estimator_normalized_linear.estimate(validation_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's make the same steps using other model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_estimator_normalized_forest=FitPredictorEstimator(prediction_strategy='random_forest_classifier',\n",
    "                                         estimation_strategy='f1_score',\n",
    "                                         specify_estimation_strategy='weighted',specify_cv=5,specify_num_estimators=50\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor_estimator_normalized_forest.cross_validate_estimate(train_dataset_no_labels_normolized,train_dataset_labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9591612302746899"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest Classifier performs better, than linear model\n",
    "predictor_estimator_normalized_forest.cross_validate_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_estimator_forest=FitPredictorEstimator(prediction_strategy='random_forest_classifier',\n",
    "                                         estimation_strategy='f1_score',\n",
    "                                         specify_estimation_strategy='weighted',specify_cv=5,specify_num_estimators=50\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_estimator_forest.cross_validate_estimate(train_dataset_no_labels,train_dataset_labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583754696757832"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us experiment a bit with random forest classifier: the class object, which uses normolized dataset was launched above, and\n",
    "#here not normolized dataset was used. As it comes to decision trees or forest the scale of the data should make no difference\n",
    "#to the result of the model, and we can see that it makes almost no. Almost, that means, that there is a slight difference between\n",
    "#these two models\n",
    "predictor_estimator_forest.cross_validate_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since random forest performs better, than sgd, and there is almost no difference between models on scaled and not scaled data, \n",
    "#as it comes to forest, let's use random forest with not scaled training data in further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_estimator_forest.fit(train_dataset_no_labels,train_dataset_labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 2, 4, ..., 2, 6, 0], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_estimator_forest.predict(validation_dataset_no_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629459774325437"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_estimator_forest.estimate(validation_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions=predictor_estimator_forest.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_dataset=pd.DataFrame(test_predictions,columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_dataset['ImageId']=[i for i in range(1,test_predictions_dataset.shape[0]+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_dataset=test_predictions_dataset[['ImageId','Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27995</td>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27996</td>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27997</td>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27998</td>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27999</td>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      4\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_dataset.to_csv(r'C:\\Users\\pc\\Desktop\\ML_projects\\Digit Recognizer Competition_result\\result_v2.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
